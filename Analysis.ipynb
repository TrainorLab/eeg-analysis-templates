{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "You will need to import MNE in order to use its functions to load and preprocess your data. Numpy is an extremely useful package used for mathematical and matrix operations. Scipy is a package for scientific computing and contains a wide range of tools for data analysis and running statistical tests. Pandas is the most convenient way of reading event codes from CSV files, but can also be used for analyzing tables of data. Matplotlib is Python's primary plotting library, and will be useful for visualizing results. The fdrcorrection() function can be imported from the statsmodels package for the purposes of adjusting p-values for multiple comparisons using the Benjamini/Hochberg method (useful for EEG analyses where you might make hundreds of comparisons). The line \"%matplotlib qt\" ensures that any time you try to plot your data, it will appear in a separate pop-up window and not just embed itself in the Jupyter notebook.\n",
    "\n",
    "Remember that if you wish to run artifact blocking, you will also need to import the run_ab function from artifact_blocking.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Preprocessing\n",
    "\n",
    "The code below just performs basic data loading, referencing, filtering, and epoching. It is designed to allow you to quickly load some data into this notebook in case you would like to test out any of the analysis functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and event codes\n",
    "# Set the path to the current session's EEG recording\n",
    "filepath = 'D://Documents/EEG_Workshop/data/Adult_08_T.mff'\n",
    "\n",
    "# Load the EEG file and extract trigger times\n",
    "eeg = mne.io.read_raw_egi(filepath, preload=True)\n",
    "eeg.rename_channels({'E129': 'Cz'})\n",
    "montage = mne.channels.make_standard_montage('GSN-HydroCel-129')\n",
    "eeg = eeg.set_montage(montage)\n",
    "events = mne.find_events(eeg)\n",
    "\n",
    "# Select only events with a specific trigger code (e.g., 1)\n",
    "# Only necessary if the number of trigger pulses in your recording does not match the number of events listed in your event code files\n",
    "events = events[events[:, 2] == 1]\n",
    "# Set the list of event code files for this session\n",
    "evcode_files = ['erica/Video_Trigger_files/DupWav6min5.csv',\n",
    "                'erica/Video_Trigger_files/DupWav6min7.csv',\n",
    "                'erica/Video_Trigger_files/DupWav6min9.csv']\n",
    "# Re-code events\n",
    "evcodes = []\n",
    "for f in evcode_files:\n",
    "    evcodes.append(pd.read_csv(f, header=None))\n",
    "evcodes = np.concatenate(evcodes).flatten()\n",
    "events[:, 2] = evcodes\n",
    "\n",
    "# Drop trigger channels once events have been extracted\n",
    "eeg = eeg.pick_types(eeg=True)\n",
    "\n",
    "# Re-reference to common average\n",
    "eeg = eeg.set_eeg_reference(ref_channels='average')\n",
    "\n",
    "# Pick desired channels\n",
    "eeg = eeg.pick_channels(['E11', 'E47', 'E98'])\n",
    "\n",
    "# .5 Hz - 40 Hz bandpass filter\n",
    "l_freq = .5\n",
    "h_freq = 40\n",
    "l_trans_bandwidth = 'auto'\n",
    "h_trans_bandwidth = 'auto'\n",
    "filter_length = 'auto'\n",
    "fir_window = 'hamming'\n",
    "phase = 'zero'\n",
    "picks = None\n",
    "eeg = eeg.filter(l_freq, h_freq, method='fir', fir_design='firwin', l_trans_bandwidth=l_trans_bandwidth, h_trans_bandwidth=h_trans_bandwidth, filter_length=filter_length, fir_window=fir_window, phase=phase, pad='reflect_limited', picks=picks)\n",
    "\n",
    "# Epoch data after defining ROIs, baseline correct each epoch by its own average\n",
    "tmin = 1.\n",
    "tmax = 29.\n",
    "baseline = (None, None)\n",
    "eeg = mne.Epochs(eeg, events, tmin=tmin, tmax=tmax, baseline=baseline, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-Related Potentials (MNE)\n",
    "\n",
    "You can calculate ERPs by calling the average() method on epoched data. ERPs are stored in a channel x time array inside of an MNE Evoked object. Each row of the data contains the ERP for a single channel, averaged over all events. \n",
    "\n",
    "If your EEG object contains epochs from multiple conditions, average() will combine all conditions into one overall ERP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp = eeg.average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get separate ERPs for each condition, you will need to index your EEG object with the name of each condition and run average() for each condition. The code below produces a list containing the Evoked objects for each condition you specify in condition_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp1 = eeg['1'].average()\n",
    "erp2 = eeg['2'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ERP Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERP Butterfly Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = erp1.plot()\n",
    "fig2 = erp2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Topomaps of ERPs\n",
    "\n",
    "Note that if you analyzing ROIs rather than channels, it will not be possible to plot a topomap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = erp1.plot_topomap()\n",
    "fig2 = erp2.plot_topomap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison of Conditions\n",
    "\n",
    "Click on any channel location to view a plot of that channel's ERP in each condition. Note that you will not be able to plot a topomap if you are analyzing ROIs instead of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mne.viz.plot_evoked_topo([erp1, erp2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-Related Potentials (Manual)\n",
    "\n",
    "While MNE is the most convenient way to calculate ERPs, it only gives you the mean of each ERP. If you want to plot your ERPs with confidence intervals or shaded error regions, you will need to calculate them yourself outside of MNE. Here is some example code for calculating the mean and standard error across events and plotting a comparison of two conditions for a given channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors for condition 1\n",
    "m1 = eeg['1'].get_data().mean(axis=0) * 1000000\n",
    "s1 = eeg['1'].get_data().std(axis=0) * 1000000 / np.sqrt(eeg['1']._data.shape[0])\n",
    "\n",
    "# Calculate means and standard errors for condition 2\n",
    "m2 = eeg['2'].get_data().mean(axis=0) * 1000000\n",
    "s2 = eeg['2'].get_data().std(axis=0) * 1000000 / np.sqrt(eeg['2']._data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_name = 'E98'\n",
    "condition_names = ['1', '2']\n",
    "color1 = 'C0'\n",
    "color2 = 'C3'\n",
    "line_opacity = 1\n",
    "error_opacity = .2\n",
    "\n",
    "# Identify index of desired channel\n",
    "ch = mne.pick_channels(eeg.ch_names, [ch_name])[0]\n",
    "\n",
    "# Create axis lines at 0 uV and time 0\n",
    "plt.axvline(0, c='k', ls='--')\n",
    "plt.axhline(0, c='k', ls='--')\n",
    "\n",
    "# Plot condition 1\n",
    "l1, = plt.plot(eeg.times, m1[ch, :], c=color1, alpha=line_opacity)\n",
    "plt.fill_between(eeg.times, m1[ch, :] - s1[ch, :], m1[ch, :] + s1[ch, :], color=color1, alpha=error_opacity)\n",
    "\n",
    "# Plot condition 2\n",
    "l2, = plt.plot(eeg.times, m2[ch, :], c=color2, alpha=line_opacity)\n",
    "plt.fill_between(eeg.times, m2[ch, :] - s2[ch, :], m2[ch, :] + s2[ch, :], color=color2, alpha=error_opacity)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title(ch_name)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (uV)')\n",
    "plt.legend([l1, l2], condition_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Frequency Decomposition\n",
    "\n",
    "## Morlet Wavelet Decomposition\n",
    "\n",
    "### Settings:\n",
    "\n",
    "The first three settings affect the decomposition process:\n",
    "\n",
    "- **freqs**: An array containing the frequencies you wish to decompose. If you wish to use linearly-spaced or log-spaced frequencies, you can use one of the numpy functions provided to generate an array of frequencies for you.\n",
    "- **n_cycles**: The number of wavelet cycles to use for each frequency. Can be an array with one value for each frequency, or a single value to use for all frequencies.\n",
    "- **picks**: Set to None to run on all channels. Set to a list of channel indices to select only those channels.\n",
    "\n",
    "The final three settings affect what information is returned:\n",
    "\n",
    "- **decim**: Sets decimation level to apply after decomposition, in order to reduce memory usage. For instance, if decimation level is set to 2, every other time point will be returned. If decimation level is set to 3, every third time point will be returned. If decimation level is 1 (default), all time points will be returned.\n",
    "- **average**: If True, returns time-frequency information averaged across all epochs. If False, returns time-frequency information for each epochs.\n",
    "- **output**: Set to \"power\" to only output power. Set to \"complex\" to return power and phase information. Note that average must be set to False to use \"complex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs = np.linspace(3, 40, num=15)  # Create linearly-spaced frequencies\n",
    "freqs = np.logspace(np.log10(3), np.log10(40), num=15)  # Create log-spaced frequencies\n",
    "n_cycles = 5\n",
    "picks = None\n",
    "decim = 1\n",
    "average = True\n",
    "output = 'power'\n",
    "power, itc = mne.time_frequency.tfr_morlet(eeg, freqs, n_cycles, decim=decim, picks=picks, average=average, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitaper Method\n",
    "\n",
    "The settings for the multitaper method of time-frequency decomposition are identical to those for Morlet wavelet decomposition, with the exception that the function cannot be set to output complex numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs = np.linspace(3, 40, num=15)  # Create linearly-spaced frequencies\n",
    "freqs = np.logspace(np.log10(3), np.log10(40), num=15)  # Create log-spaced frequencies\n",
    "n_cycles = 5\n",
    "picks = None\n",
    "decim = 1\n",
    "average = True\n",
    "power, itc = mne.time_frequency.tfr_multitaper(eeg, freqs, n_cycles, decim=decim, picks=picks, average=average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Tagging\n",
    "\n",
    "The code below implements the frequency-tagging procedure from Nozaradan, Peretz, Missal, and Mouraux (2011). The steps involve 1) running the Fast Fourier Transform on your average data, 2) subtracting from each frequency's amplitude the average amplitude of its neighboring frequencies, and 3) replacing each frequency's amplitude with the average amplitude of itself and its neighbors.\n",
    "\n",
    "This process requires the following settings and provides you with the following outputs:\n",
    "### Settings:\n",
    "- **data**: A 2D channels x time array containing the average time series for each channel. Note that if your averaged data is stored as an mne.Evoked object, you will need to extract the data array from the object as shown.\n",
    "- **nfreqs**: An integer indicating how many frequencies to decompose via FFT. Note that half of the frequencies produced by FFT will be negative. As we only care about nonnegative frequencies, the code will automatically drop the negative frequencies, leaving you with only half *nfreqs* frequencies.\n",
    "- **sampling_rate**: A float indicating the number of samples recorded per second.\n",
    "\n",
    "### Outputs:\n",
    "- **freqs**: A 1D array of the frequencies decomposed by the FFT.\n",
    "- **amp**: A 2D channels x frequencies array containing each channel's raw amplitude at each frequency. \n",
    "- **norm_amp**: A 2D channels x frequencies array containing each channel's amplitude at each frequency after normalization.\n",
    "\n",
    "If you only care about a specific few frequencies of interest, you can use the code from the second cell below to return each channel's normalized amplitude at each of those target frequencies. In the event that the exact target frequencies were not among those decomposed by the FFT, the nearest possible frequency will be used instead (although you should try to set up your FFT so that the exact frequencies of interest are analyzed). This process requires you to specify one setting and provides you with the following two outputs:\n",
    "\n",
    "### Settings:\n",
    "- **target_freqs**: A list of the frequencies of interest.\n",
    "\n",
    "### Outputs:\n",
    "- **closest_freqs**: A 1D array of the closest frequencies to the target frequencies.\n",
    "- **target_amp**: A 2D channels x frequencies array containing the each channel's amplitude at each target frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT Settings\n",
    "data = erp1.data\n",
    "nfreqs = data.shape[1]\n",
    "sampling_rate = erp1.info['sfreq']\n",
    "\n",
    "# Run FFT\n",
    "freqs = np.fft.fftfreq(nfreqs, d=1/sampling_rate)\n",
    "amp = np.abs(np.fft.fft(data, n=nfreqs, axis=1))\n",
    "amp = amp[:, freqs >= 0]\n",
    "freqs = freqs[freqs >= 0]\n",
    "\n",
    "# Subtract average of bins +/- 3-5\n",
    "sub_amp = np.empty_like(amp)\n",
    "reference_bins = np.array([-5, -4, -3, 3, 4, 5])\n",
    "for i in range(sub_amp.shape[1]):\n",
    "    reference_inds = reference_bins + i\n",
    "    reference_inds = reference_inds[(reference_inds >= 0) & (reference_inds < amp.shape[1])]\n",
    "    sub_amp[:, i] = amp[:, i] - np.mean(amp[:, reference_bins], axis=1)\n",
    "sub_amp[sub_amp < 0] = 0  # Don't allow negative amplitudes\n",
    "\n",
    "# Average amplitudes across neighboring bins\n",
    "norm_amp = np.zeros((sub_amp.shape[0], len(freqs)))\n",
    "bins_to_avg = np.array([-1, 0, 1])\n",
    "for i in range(sub_amp.shape[1]):\n",
    "    inds_to_avg = bins_to_avg + i\n",
    "    inds_to_avg = inds_to_avg[(inds_to_avg >= 0) & (inds_to_avg < sub_amp.shape[1])]\n",
    "    norm_amp[:, i] = np.mean(sub_amp[:, inds_to_avg], axis=1)\n",
    "del sub_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get amplitudes from list of target frequencies\n",
    "target_freqs = [.8, 1.2, 2.4]\n",
    "\n",
    "closest_freqs = np.empty(len(target_freqs))\n",
    "target_amp = np.empty((amp.shape[0], len(target_freqs)))\n",
    "for i, f in enumerate(target_freqs):\n",
    "    ind = np.argmin(np.abs(freqs - f))\n",
    "    closest_freqs[i] = freqs[ind]\n",
    "    target_amp[i] = amp[:, ind]\n",
    "    \n",
    "print('Frequencies:\\n', closest_freqs)\n",
    "print('Amplitudes:\\n', target_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you'll find some basic example code for plotting power spectra before and after frequency tagging for three regions of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 0\n",
    "max_freq = 20\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title('Raw')\n",
    "plt.ylabel(erp1.ch_names[0])\n",
    "plt.plot(freqs, amp[0, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "plt.subplot(322)\n",
    "plt.title('Normalized')\n",
    "plt.plot(freqs, norm_amp[0, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.ylabel(erp1.ch_names[1])\n",
    "plt.plot(freqs, amp[1, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "plt.subplot(324)\n",
    "plt.plot(freqs, norm_amp[1, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.ylabel(erp1.ch_names[2])\n",
    "plt.plot(freqs, amp[2, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "plt.subplot(326)\n",
    "plt.plot(freqs, norm_amp[2, :])\n",
    "plt.xlim(min_freq, max_freq)\n",
    "\n",
    "plt.gcf().set_size_inches((12, 10))\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "Python offers convenient functions for running most basic statistics (t-tests, correlations, etc.). While you can perform more advanced statistics in Python as well, it is recommended that you export your data into R or SPSS if you wish to run ANOVAs or anything more complex. \n",
    "\n",
    "### Simulated Data (for test-running stats functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random ERPs with 1000 time points from 30 subjects and 2 conditions (data should be organized with one subject per row)\n",
    "cond1 = np.random.randn(30, 1000)\n",
    "cond2 = np.random.randn(30, 1000)\n",
    "\n",
    "# Simulate 100 data points for two measures\n",
    "score1 = np.random.randn(100)\n",
    "score2 = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Samples *t*-Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each time point between conditions, giving one t and p-value per time point\n",
    "tvals, pvals = ss.ttest_ind(cond1, cond2)\n",
    "\n",
    "# Identify time points that differ signficantly between conditions after Benjamini-Hochberg FDR correction\n",
    "sig, pvals_adjusted = fdrcorrection(pvals, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Samples *t*-Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each time point between conditions, giving one t and p-value per time point\n",
    "tvals, pvals = ss.ttest_rel(cond1, cond2)\n",
    "\n",
    "# Identify time points that differ signficantly between conditions after Benjamini-Hochberg FDR correction\n",
    "sig, pvals_adjusted = fdrcorrection(pvals, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson r between two sets of scores\n",
    "rval, pval = ss.pearsonr(score1, score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: You have 30 participants with 5 dependent measures and 2 conditions\n",
    "data = np.random.randn(30, 5)\n",
    "conditions = np.random.choice([1, 2], 30)\n",
    "col_names = ['score1', 'score2', 'score3', 'score4', 'score5']\n",
    "\n",
    "# Organize your data as a dataframe\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "# Add column indicating which participants were in which condition\n",
    "df['condition'] = conditions\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('C://Users/jpazd/Downloads/example_data.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
